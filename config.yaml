model:
  name: "Mozhi"
  description: "English to Tamil Translator"
  path: './weights/mozhi.pt'
  parameters:
    dim_model: 512
    d_ff: 2048
    src_vocab_size: -1
    tgt_vocab_size: -1
    num_heads: 8
    num_layers: 6
    dropout: 0.1
    seq_len: 512

tokenizer:
  data_path: ./data/dataset_large.parquet
  src:
    special_tokens:
      bos_token: "<s>"
      eos_token: "</s>"
      sep_token: "</s>"
      cls_token: "<s>"
      pad_token: "<pad>"
      mask_token: "<mask>"
      unk_token: "<unk>"
    tokenizer_path: './weights/tokenizer_en.json'
    vocab_size: 50265
    min_frequency: 2
    lang: 'en'
  tgt:
    special_tokens:
      bos_token: "<s>"
      eos_token: "</s>"
      sep_token: "</s>"
      cls_token: "<s>"
      pad_token: "<pad>"
      mask_token: "<mask>"
      unk_token: "<unk>"
    tokenizer_path: './weights/tokenizer_ta.json'
    vocab_size: 50265
    min_frequency: 2
    lang: 'ta'

data:
  src: 'en'
  tgt: 'ta'
  data_dir: './data'
  src_to_tgt: # src : tgt
    'train1.en.txt': 'train1.ta.txt'
    'train2.en.txt': 'train2.ta.txt'
    'train3.en.txt': 'train3.ta.txt'

train:
  dataset: './data/dataset.csv'
  device: 'cuda'
  batch_size: 32
  epochs: 10
  label_smoothing: 0.1
  optimizer:
    lr: 0.0001
    eps: 0.00000000001
  finetune: false
  logging:
    dir: './logs'

inference:
  device: 'cuda'