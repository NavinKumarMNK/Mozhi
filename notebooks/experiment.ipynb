{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "import transformers\n",
    "\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from typing import Generator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import (\n",
    "    Tokenizer, \n",
    "    models, \n",
    "    normalizers, \n",
    "    pre_tokenizers, \n",
    "    decoders, \n",
    "    trainers, \n",
    "    processors\n",
    ")\n",
    "\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "with open('../config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train1.en.txt': 'train1.ta.txt',\n",
       " 'train2.en.txt': 'train2.ta.txt',\n",
       " 'train3.en.txt': 'train3.ta.txt'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_config = config['data']\n",
    "docs = data_config['src_to_tgt']\n",
    "src_lang = data_config['src']\n",
    "tgt_lang = data_config['tgt']\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That's what I am saying.</td>\n",
       "      <td>என்றுதான் நான் சொல்ல வருகிறேன்.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Every tournament is difficult.</td>\n",
       "      <td>ஒவ்வொரு சுற்றுப்பயணமும் கடினமானது.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the first questions Flavio posed was, D...</td>\n",
       "      <td>பல வருடங்களாக அவர் அந்த நித்திய எரிநரக தண்டனைய...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He gave full credit to the Union Finance Minis...</td>\n",
       "      <td>அவர் நிதி அமைச்சர் அருண்ஜேட்லியின் முயற்சியை த...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some art historians have suggested that he onl...</td>\n",
       "      <td>சில கலை வரலாற்றாசிரியர்கள் அவர் ஒரு வருடத்திற்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198656</th>\n",
       "      <td>mental</td>\n",
       "      <td>மன</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198657</th>\n",
       "      <td>mental aberration</td>\n",
       "      <td>மனப் பிறழ்ச்சி</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198658</th>\n",
       "      <td>mental competency</td>\n",
       "      <td>மனத் தேர்ச்சி</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198659</th>\n",
       "      <td>mental deficiency</td>\n",
       "      <td>மன ஊனம்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198660</th>\n",
       "      <td>mention</td>\n",
       "      <td>குறிப்பிடு</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5198661 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        en  \\\n",
       "0                                 That's what I am saying.   \n",
       "1                           Every tournament is difficult.   \n",
       "2        One of the first questions Flavio posed was, D...   \n",
       "3        He gave full credit to the Union Finance Minis...   \n",
       "4        Some art historians have suggested that he onl...   \n",
       "...                                                    ...   \n",
       "5198656                                             mental   \n",
       "5198657                                  mental aberration   \n",
       "5198658                                  mental competency   \n",
       "5198659                                  mental deficiency   \n",
       "5198660                                            mention   \n",
       "\n",
       "                                                        ta  \n",
       "0                          என்றுதான் நான் சொல்ல வருகிறேன்.  \n",
       "1                       ஒவ்வொரு சுற்றுப்பயணமும் கடினமானது.  \n",
       "2        பல வருடங்களாக அவர் அந்த நித்திய எரிநரக தண்டனைய...  \n",
       "3        அவர் நிதி அமைச்சர் அருண்ஜேட்லியின் முயற்சியை த...  \n",
       "4        சில கலை வரலாற்றாசிரியர்கள் அவர் ஒரு வருடத்திற்...  \n",
       "...                                                    ...  \n",
       "5198656                                                 மன  \n",
       "5198657                                     மனப் பிறழ்ச்சி  \n",
       "5198658                                      மனத் தேர்ச்சி  \n",
       "5198659                                            மன ஊனம்  \n",
       "5198660                                         குறிப்பிடு  \n",
       "\n",
       "[5198661 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "    return lines\n",
    "\n",
    "dfs = []\n",
    "for key, val in docs.items():\n",
    "    txt1, txt2 = (read_text_file(os.path.join(data_config['data_dir'], key)), \n",
    "                  read_text_file(os.path.join(data_config['data_dir'], val)))\n",
    "    corpus = pd.DataFrame({'src': txt1, 'tgt': txt2})    \n",
    "    dfs.append(corpus)\n",
    "\n",
    "corpus = pd.concat(dfs, ignore_index=True)\n",
    "corpus.rename(columns={'src': data_config['src'], 'tgt': data_config['tgt']}, inplace=True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus.to_parquet(os.path.join(data_config['data_dir'], 'dataset_large.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_bpe_tokenizer(tokenizer: Tokenizer, series, config):\n",
    "    tokenizer_path = Path(config['tokenizer_path'])\n",
    "    special_tokens = {\n",
    "        config['special_tokens']['bos_token']: 0,\n",
    "        config['special_tokens']['pad_token']: 1,\n",
    "        config['special_tokens']['eos_token']: 2,\n",
    "        config['special_tokens']['unk_token']: 3,\n",
    "        config['special_tokens']['mask_token']: config['vocab_size'] - 1,\n",
    "    }\n",
    "    \n",
    "    if config['lang'] == 'ta':\n",
    "        normalizer = normalizers.NFKC()\n",
    "        pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "        decoder = decoders.Metaspace()\n",
    "    elif config['lang'] == 'en':\n",
    "        normalizer = normalizers.Sequence([\n",
    "            normalizers.NFKC(),\n",
    "            normalizers.Lowercase()\n",
    "        ])\n",
    "        pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "        decoder = decoders.ByteLevel()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {config['lang']}\")\n",
    "    \n",
    "    post_processor = processors.TemplateProcessing(\n",
    "        single=f\"{config['special_tokens']['bos_token']} $A {config['special_tokens']['eos_token']}\",\n",
    "        special_tokens=list(special_tokens.items()),\n",
    "    )\n",
    "    \n",
    "    trainer = trainers.BpeTrainer(\n",
    "        special_tokens=list(special_tokens.keys()),\n",
    "        vocab_size=config['vocab_size'],\n",
    "        min_frequency=config['min_frequency'],\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "    tokenizer.normalizer = normalizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizer\n",
    "    tokenizer.decoder = decoder\n",
    "    tokenizer.post_processor = post_processor\n",
    "    \n",
    "    def get_sentences(series: pd.Series) -> Generator[str, None, None]:\n",
    "        for text in series:\n",
    "            yield text\n",
    "\n",
    "    tokenizer.train_from_iterator(\n",
    "        get_sentences(series=series),\n",
    "        trainer=trainer,\n",
    "        length=len(series),\n",
    "    )\n",
    "\n",
    "    tokenizer.save(str(tokenizer_path))\n",
    "    print(tokenizer)\n",
    "    print(tokenizer.get_vocab_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "<tokenizers.Tokenizer object at 0x55b15a282a30>\n",
      "10000\n",
      "[0, 171, 331, 273, 189, 335, 1878, 18, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['<s>', 'Ġthat', \"'s\", 'Ġwhat', 'Ġi', 'Ġam', 'Ġsaying', '.', '</s>']\n",
      "[]\n",
      "decoded_string = \" that's what i am saying.\"\n",
      "Size of vocabulary: 10000\n",
      "Successfully trained tokenizer <tokenizers.Tokenizer object at 0x55b15a282a30>\n",
      "\n",
      "\n",
      "\n",
      "<tokenizers.Tokenizer object at 0x55b14e49db40>\n",
      "10000\n",
      "[0, 2839, 3272, 2959, 3221, 3409, 2952, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['<s>', '▁என்று', 'தான்', '▁நான்', '▁சொல்ல', '▁வருகிற', 'ேன்.', '</s>']\n",
      "[]\n",
      "decoded_string = 'என்றுதான் நான் சொல்ல வருகிறேன்.'\n",
      "Size of vocabulary: 10000\n",
      "Successfully trained tokenizer <tokenizers.Tokenizer object at 0x55b14e49db40>\n"
     ]
    }
   ],
   "source": [
    "with open('../config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "config = config['tokenizer']\n",
    "df = pd.read_parquet(config['data_path'])\n",
    "\n",
    "for items in (config['src'], config['tgt']):\n",
    "    _df = df[items['lang']]\n",
    "\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=items['special_tokens']['unk_token']))\n",
    "    train_bpe_tokenizer(tokenizer, _df, items)\n",
    "\n",
    "    encoded_tokens = tokenizer.encode(_df[0])\n",
    "    print(encoded_tokens.ids)\n",
    "    print(encoded_tokens.type_ids)\n",
    "    print(encoded_tokens.tokens)\n",
    "    print(encoded_tokens.overflowing)\n",
    "\n",
    "    encoded_ids = encoded_tokens.ids\n",
    "\n",
    "    decoded_string = tokenizer.decode(encoded_ids)\n",
    "    print(f\"{decoded_string = }\")\n",
    "\n",
    "    print(\"Size of vocabulary:\", tokenizer.get_vocab_size())\n",
    "    print(\"Successfully trained tokenizer\", tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2881, 3673, 3749, 9110, 945, 6946, 960, 3227, 6038, 975, 3406, 5781, 2683, 3803, 3520, 8450, 3117, 4453, 4402, 3593, 3148, 7031, 4120, 3433, 5209, 7311, 2715, 4570, 2884, 2703, 3069, 2813, 4490, 2778, 3406, 4294, 4901, 3092, 6293, 7169, 3011, 2773, 2953, 3877, 3633, 2751, 2771, 962, 3375, 3195, 3663, 2695, 3007, 8095, 4757, 5183, 2750, 3954, 7464, 9429, 5366, 2857, 2784, 7791, 2687, 6747, 3154, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['<s>', '▁அவர்', '▁நிதி', '▁அமைச்சர்', '▁அருண்', 'ஜ', 'ேட்', 'ல', 'ியின்', '▁முயற்சிய', 'ை', '▁தொழில்', '▁உற்பத்திய', 'ில்', '▁ஈடுப', 'ட்டுள்ள', '▁மாநிலங்கள்', '▁மத்திய', '▁அரசின்', '▁உதவிய', 'ைப்', '▁ஈ', 'டாக', '▁பெறு', 'வதற்கு', '▁ஏற்ற', '▁கட்டளை', 'வி', 'தியை', '▁அமை', 'த்து', 'க்கொ', 'டுத்த', 'ற்கும்', '▁மற்றும்', '▁தொழில்', '▁உற்பத்தி', '▁இல்லாத', '▁மாநில', 'ங்களுடன்', '▁தாங்கள்', '▁சம', 'மாக', '▁வை', 'க்கப்பட்டுள்ள', 'ார்கள்', '▁என்ற', '▁கா', 'ழ', '்ப்பு', '▁உண', 'ர்ச்சி', 'க்கு', '▁இட', 'மில்லா', 'மல்', '▁இருப்பத', 'ற்க', 'ேற்ற', '▁நிலையை', '▁ஏற்படுத்திய', 'தற்காக', 'வும்', '▁வெ', 'குவ', 'ாக', '▁பாராட்ட', 'ினார்.', '</s>']\n",
      "[]\n",
      "decoded_string = 'அவர் நிதி அமைச்சர் அருண்ஜேட்லியின் முயற்சியை தொழில் உற்பத்தியில் ஈடுபட்டுள்ள மாநிலங்கள் மத்திய அரசின் உதவியைப் ஈடாக பெறுவதற்கு ஏற்ற கட்டளைவிதியை அமைத்துக்கொடுத்தற்கும் மற்றும் தொழில் உற்பத்தி இல்லாத மாநிலங்களுடன் தாங்கள் சமமாக வைக்கப்பட்டுள்ளார்கள் என்ற காழ்ப்பு உணர்ச்சிக்கு இடமில்லாமல் இருப்பதற்கேற்ற நிலையை ஏற்படுத்தியதற்காகவும் வெகுவாக பாராட்டினார்.'\n",
      "Size of vocabulary: 10000\n",
      "Successfully trained tokenizer <tokenizers.Tokenizer object at 0x55b14e49db40>\n"
     ]
    }
   ],
   "source": [
    "encoded_tokens = tokenizer.encode(_df[3])\n",
    "print(encoded_tokens.ids)\n",
    "print(encoded_tokens.type_ids)\n",
    "print(encoded_tokens.tokens)\n",
    "print(encoded_tokens.overflowing)\n",
    "\n",
    "encoded_ids = encoded_tokens.ids\n",
    "\n",
    "decoded_string = tokenizer.decode(encoded_ids)\n",
    "print(f\"{decoded_string = }\")\n",
    "\n",
    "print(\"Size of vocabulary:\", tokenizer.get_vocab_size())\n",
    "print(\"Successfully trained tokenizer\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5198661 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 1366695/5198661 [02:14<06:18, 10125.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Mozhi/notebooks/experiment.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6b696e645f6a657073656e227d/workspace/Mozhi/notebooks/experiment.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m dct \u001b[39m=\u001b[39m defaultdict(\u001b[39mint\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6b696e645f6a657073656e227d/workspace/Mozhi/notebooks/experiment.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m tqdm(_df):\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6b696e645f6a657073656e227d/workspace/Mozhi/notebooks/experiment.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     encoded_tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode(text)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6b696e645f6a657073656e227d/workspace/Mozhi/notebooks/experiment.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m encoded_tokens\u001b[39m.\u001b[39mtokens:\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6b696e645f6a657073656e227d/workspace/Mozhi/notebooks/experiment.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         dct[token] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dct = defaultdict(int)\n",
    "for text in tqdm(_df):\n",
    "    encoded_tokens = tokenizer.encode(text)\n",
    "    for token in encoded_tokens.tokens:\n",
    "        dct[token] += 1\n",
    "\n",
    "dct = dict(sorted(dct.items(), key=lambda x: x[1], reverse=True))\n",
    "dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inspecting BART Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartTokenizer(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>என்றுதான் நான் சொல்ல வருகிறேன்.</s>'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tokenizer.decode(tokenizer.encode(_df[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      " \n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      " \n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      " \n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      "�\n",
      ".\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "lst = _tokenizer.encode(_df[0])\n",
    "for item in lst:\n",
    "    print(_tokenizer.decode(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (src_emb): InputEmbeddings(\n",
      "    (embedding): Embedding(10000, 256)\n",
      "  )\n",
      "  (tgt_emb): InputEmbeddings(\n",
      "    (embedding): Embedding(10000, 256)\n",
      "  )\n",
      "  (src_pos): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (tgt_pos): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0-5): 6 x EncoderBlock(\n",
      "        (multi_head_attention): MultiHeadAttentionBlock(\n",
      "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ffn): FeedForwardBlock(\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm_ffn): LayerNorm()\n",
      "        (norm_mha): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (decoder_layers): ModuleList(\n",
      "      (0-5): 6 x DecoderBlock(\n",
      "        (self_attention): MultiHeadAttentionBlock(\n",
      "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (cross_attention): MultiHeadAttentionBlock(\n",
      "          (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (w_o): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ffn): FeedForwardBlock(\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (4): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm_self): LayerNorm()\n",
      "        (norm_cross): LayerNorm()\n",
      "        (norm_ffn): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projection): ProjectionHead(\n",
      "    (proj): Linear(in_features=256, out_features=10000, bias=True)\n",
      "  )\n",
      ")\n",
      "Encoder Parameters: 4,732,442\n",
      "Decoder Parameters: 6,311,462\n",
      "Embedding Parameters: 2,560,000\n",
      "Embedding Parameters: 2,560,000\n",
      "Positional Encoding Parameters: 0\n",
      "Positional Encoding Parameters: 0\n",
      "Projection Parameters: 2,570,000\n"
     ]
    }
   ],
   "source": [
    "# Author : NavinKumarMNK\n",
    "\"\"\"Transformer Model\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(seed=SEED)\n",
    "torch.cuda.manual_seed_all(seed=SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    # stores embedding of the tokens\n",
    "    def __init__(self, dim_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size, embedding_dim=self.dim_model\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.embedding(x) * math.sqrt(self.dim_model)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # calculate the positional embedding\n",
    "    def __init__(self, dim_model: int, seq_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # PE => (seq_len, d_model) ; position, div_term => (self.seq_len, 1)\n",
    "        position_encoding = torch.zeros(size=(seq_len, dim_model))\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, dim_model, 2).float() * (-math.log(10000.0) / dim_model)\n",
    "        )  # e^(2*i * (ln 10000) / dim_model)\n",
    "\n",
    "        # sin() to even pos & cos() to odd position\n",
    "        position_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        position_encoding = position_encoding.unsqueeze(0)  # (1, seq_len, dim_model)\n",
    "\n",
    "        # register buffer => Keep with module but not as learnable paramter\n",
    "        self.register_buffer(\"position_encoding\", position_encoding)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # positional encodings are added only till the valid tokens in x : (batch_size, seq_len, dim)\n",
    "        return self.dropout(\n",
    "            x + (self.position_encoding[:, : x.shape[1], :]).requires_grad_(False)\n",
    "        )\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, input_size, ):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size,\n",
    "                      out_channels=input_size // 2 , \n",
    "                      kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=input_size//2, \n",
    "                       out_channels=input_size//4, \n",
    "                       kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    # Normazalize across Layers => Xj = (xj - uj)/(sigma^2 + e)^(0.5)\n",
    "    def __init__(self, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        # parameters: alpha (multiplicative) bias (additive)\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return (\n",
    "            self.alpha\n",
    "            * (x - x.mean(-1, keepdim=True))\n",
    "            / (self.eps + x.std(dim=-1, keepdim=True))\n",
    "        ) + self.bias\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    # sequence of linear layer : [dim -> ddf(general-4*dim) -> dim]\n",
    "    def __init__(self, dim_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Linear -> Norm -> Activation -> Dropout -> Linear\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, dim_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.ffn(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim_model: int, num_heads: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            self.dim_model % self.num_heads == 0\n",
    "        ), \"dim_model is not divisible by num_heads\"\n",
    "\n",
    "        self.d_k = self.dim_model // self.num_heads\n",
    "\n",
    "        # key, query, value\n",
    "        self.w_q = nn.Linear(self.dim_model, self.dim_model)\n",
    "        self.w_k = nn.Linear(self.dim_model, self.dim_model)\n",
    "        self.w_v = nn.Linear(self.dim_model, self.dim_model)\n",
    "\n",
    "        # concat([heads]) * w_o\n",
    "        self.w_o = nn.Linear(self.dim_model, self.dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        dropout: Optional[nn.Dropout] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        # attention = q.k / sqrt(dim) : (batch_szie, h, seq_len, d_k) -> (_, _, _, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return attention_scores @ value\n",
    "\n",
    "    # mask => control attention by blocking interactions between two words\n",
    "    def forward(self, q, k, v, mask: Optional[torch.Tensor]):\n",
    "        # (batch_size, seq_len, dim_model) -> (batch_size, seq_len, dim_model) ->\n",
    "        # (batch_size, seq_len, heads, d_k) -> (batch_size, heads, seq_len, d_k) : process across heads\n",
    "        query = (\n",
    "            self.w_q(q)\n",
    "            .view(query.shape[0], query.shape[1], self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        key = (\n",
    "            self.w_k(k)\n",
    "            .view(key.shape[0], key.shape[1], self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        value = (\n",
    "            self.w_v(v)\n",
    "            .view(value.shape[0], value.shape[1], self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        # final multihead attentions\n",
    "        x = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # (batch_size, heads, seq_len, d_k) -> (_, seq_len, heads, _) -> (batch_size, seq_len, dim_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.dim_model)\n",
    "        x = self.w_o(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    # pre_norm -> mha() -> residual(before norm) -> pre_norm -> ffn() -> residual(before norm)\n",
    "    def __init__(\n",
    "        self, dim_model: int, num_heads: int, dropout: float, d_ff: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttentionBlock(\n",
    "            dim_model=dim_model, num_heads=num_heads, dropout=dropout\n",
    "        )\n",
    "        self.ffn = FeedForwardBlock(dim_model=dim_model, d_ff=d_ff, dropout=dropout)\n",
    "\n",
    "        self.norm_ffn = LayerNorm()\n",
    "        self.norm_mha = LayerNorm()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        x_norm = self.norm_mha(x)\n",
    "        x_atten = x + self.multi_head_attention(x_norm, x_norm, x_norm, mask)\n",
    "        x_ffn = x_atten + self.ffn(self.norm_ffn(x_atten))\n",
    "        return x_ffn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    # stacked n EncoderBlocks\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        d_ff: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm = LayerNorm()\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            modules=[\n",
    "                EncoderBlock(\n",
    "                    dim_model=dim_model, num_heads=num_heads, dropout=dropout, d_ff=d_ff\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.layer_norm(x)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    # pre_norm -> masked mha (self-attention) -> residual -> pre_norm(q) + encoder(k, v) ->\n",
    "    # mha (cross-attention) -> residual -> pre_norm -> ffn -> residual\n",
    "    def __init__(\n",
    "        self, dim_model: int, num_heads: int, dropout: float, d_ff: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttentionBlock(\n",
    "            dim_model=dim_model, num_heads=num_heads, dropout=dropout\n",
    "        )\n",
    "        self.cross_attention = MultiHeadAttentionBlock(\n",
    "            dim_model=dim_model, num_heads=num_heads, dropout=dropout\n",
    "        )\n",
    "        self.ffn = FeedForwardBlock(dim_model=dim_model, d_ff=d_ff, dropout=dropout)\n",
    "\n",
    "        self.norm_self = LayerNorm()\n",
    "        self.norm_cross = LayerNorm()\n",
    "        self.norm_ffn = LayerNorm()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_x: torch.Tensor,\n",
    "        src_mask: Optional[torch.Tensor],\n",
    "        tgt_mask: Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        x_norm = self.norm_self(x)\n",
    "        x = x + self.self_attention(x_norm, x_norm, x_norm, tgt_mask)\n",
    "\n",
    "        x_norm = self.norm_cross(x)\n",
    "        x = x + self.cross_attention(x, encoder_x, encoder_x, src_mask)\n",
    "\n",
    "        x_norm = self.norm_ffn(x)\n",
    "        x = x + self.ffn(x_norm)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # stacked n DecoderBlocks\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        d_ff: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # module list of n Decoder Blocks\n",
    "        self.layer_norm = LayerNorm()\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            modules=[\n",
    "                DecoderBlock(\n",
    "                    dim_model=dim_model, num_heads=num_heads, dropout=dropout, d_ff=d_ff\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        return self.layer_norm(x)\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    # feature vector to vocab\n",
    "    def __init__(self, dim_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        # (batch_size, seq_len, d_model) -> (_, _, vocab_size)\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        d_ff: int,\n",
    "        src_max_seq_len: int,\n",
    "        tgt_max_seq_len: int,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.src_max_seq_len = src_max_seq_len\n",
    "        self.tgt_max_seq_len = tgt_max_seq_len\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "\n",
    "        # Embeddings (vocab -> vector)\n",
    "        self.src_emb = InputEmbeddings(\n",
    "            dim_model=self.dim_model, vocab_size=self.src_vocab_size\n",
    "        )\n",
    "        self.tgt_emb = InputEmbeddings(\n",
    "            dim_model=self.dim_model, vocab_size=self.tgt_vocab_size\n",
    "        )\n",
    "\n",
    "        self.src_pos = PositionalEncoding(\n",
    "            dim_model=self.dim_model, seq_len=self.src_max_seq_len, dropout=self.dropout\n",
    "        )\n",
    "        self.tgt_pos = PositionalEncoding(\n",
    "            dim_model=self.dim_model, seq_len=self.tgt_max_seq_len, dropout=self.dropout\n",
    "        )\n",
    "        \n",
    "        # Core Layers\n",
    "        self.encoder = Encoder(\n",
    "            dim_model=self.dim_model,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.dropout,\n",
    "            num_heads=self.num_heads,\n",
    "            d_ff=self.d_ff,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            dim_model=self.dim_model,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=self.dropout,\n",
    "            num_heads=self.num_heads,\n",
    "            d_ff=self.d_ff,\n",
    "        )\n",
    "\n",
    "        # Conversion head (vector -> word)\n",
    "        self.projection = ProjectionHead(\n",
    "            dim_model=self.dim_model, vocab_size=self.tgt_vocab_size\n",
    "        )\n",
    "\n",
    "        for params in self.parameters():\n",
    "            if params.dim() > 1:\n",
    "                nn.init.xavier_uniform_(params)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_emb(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, tgt, src_output, src_mask, tgt_mask):\n",
    "        tgt = self.tgt_emb(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, src_output, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.projection(x)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src_output = self.encode(src, src_mask)\n",
    "        tgt_output = self.decode(tgt, src_output, src_mask, tgt_mask)\n",
    "        return self.project(tgt_output)\n",
    "\n",
    "\n",
    "with open(\"../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model = Transformer(**config[\"model\"][\"parameters\"])    \n",
    "print(model)\n",
    "\n",
    "# calculate no of parameters in encoder, decoder, and embedding layers separately\n",
    "# encoder\n",
    "encoder_params = sum(p.numel() for p in model.encoder.parameters() if p.requires_grad)\n",
    "print(f\"Encoder Parameters: {encoder_params:,}\")\n",
    "\n",
    "# decoder\n",
    "decoder_params = sum(p.numel() for p in model.decoder.parameters() if p.requires_grad)\n",
    "print(f\"Decoder Parameters: {decoder_params:,}\")\n",
    "\n",
    "# embedding\n",
    "embedding_params = sum(p.numel() for p in model.src_emb.parameters() if p.requires_grad)\n",
    "print(f\"Embedding Parameters: {embedding_params:,}\")\n",
    "\n",
    "# embedding target \n",
    "embedding_params = sum(p.numel() for p in model.tgt_emb.parameters() if p.requires_grad)\n",
    "print(f\"Embedding Parameters: {embedding_params:,}\")\n",
    "\n",
    "pos_parms = sum(p.numel() for p in model.src_pos.parameters() if p.requires_grad)\n",
    "print(f\"Positional Encoding Parameters: {pos_parms:,}\")\n",
    "\n",
    "pos_parms = sum(p.numel() for p in model.tgt_pos.parameters() if p.requires_grad)\n",
    "print(f\"Positional Encoding Parameters: {pos_parms:,}\")\n",
    "\n",
    "\n",
    "# projection\n",
    "projection_params = sum(p.numel() for p in model.projection.parameters() if p.requires_grad)\n",
    "print(f\"Projection Parameters: {projection_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4678794 519867\n",
      "Dataset setup complete\n",
      "{'encoder_input': tensor([[  0,   0, 281,  ...,   1,   1,   1],\n",
      "        [  0,   0, 165,  ...,   1,   1,   1],\n",
      "        [  0,   0, 143,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [  0,   0,  96,  ...,   1,   1,   1],\n",
      "        [  0,   0, 143,  ...,   1,   1,   1],\n",
      "        [  0,   0, 742,  ...,   1,   1,   1]]), 'decoder_input': tensor([[   0,    0, 3643,  ...,    1,    1,    1],\n",
      "        [   0,    0, 3945,  ...,    1,    1,    1],\n",
      "        [   0,    0, 3704,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   0,    0, 9753,  ...,    1,    1,    1],\n",
      "        [   0,    0, 3676,  ...,    1,    1,    1],\n",
      "        [   0,    0, 7880,  ...,    1,    1,    1]]), 'encoder_mask': tensor([[[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]]], dtype=torch.int32), 'decoder_mask': tensor([[[[0, 1, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 0,  ..., 1, 1, 1],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 1, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 0,  ..., 1, 1, 1],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 1, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 0,  ..., 1, 1, 1],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 1, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0, 1, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 0,  ..., 1, 1, 1],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 1, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 0,  ..., 1, 1, 1],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 1, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 1,  ..., 1, 1, 1],\n",
      "          [0, 0, 0,  ..., 1, 1, 1],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 1, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 1],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]]]), 'label': tensor([[   0, 3643, 4844,  ...,    1,    1,    1],\n",
      "        [   0, 3945, 4444,  ...,    1,    1,    1],\n",
      "        [   0, 3704, 5694,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   0, 9753, 3848,  ...,    1,    1,    1],\n",
      "        [   0, 3676, 3046,  ...,    1,    1,    1],\n",
      "        [   0, 7880, 5333,  ...,    1,    1,    1]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer_src: str, tokenizer_tgt: str, src_lang: str, \n",
    "                 tgt_lang: str, src_seq_len: int, tgt_seq_len: int ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.src_seq_len = src_seq_len\n",
    "        self.tgt_seq_len = tgt_seq_len\n",
    "\n",
    "        self.tokenizer_src: Tokenizer = Tokenizer.from_file(self.tokenizer_src)\n",
    "        self.tokenizer_tgt: Tokenizer = Tokenizer.from_file(self.tokenizer_tgt)\n",
    "\n",
    "        self.sos_token_src = self.sos_token_tgt = torch.tensor([0], dtype=torch.int64)\n",
    "        self.pad_token_src = self.pad_token_tgt = torch.tensor([1], dtype=torch.int64)\n",
    "        self.eos_token_src = self.eos_token_tgt = torch.tensor([2], dtype=torch.int64)\n",
    "        self.mask_token_src = torch.tensor([self.tokenizer_src.get_vocab_size() - 1], dtype=torch.int64)  \n",
    "        self.mask_token_tgt = torch.tensor([self.tokenizer_tgt.get_vocab_size() - 1], dtype=torch.int64) \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        # support for only DataFrame pair\n",
    "        src_text, tgt_text = self.df.iloc[idx][self.src_lang], self.df.iloc[idx][self.tgt_lang]\n",
    "        src_input_ids, tgt_input_ids = self.tokenizer_src.encode(src_text).ids, self.tokenizer_tgt.encode(tgt_text).ids\n",
    "        src_pad_len, tgt_pad_len = self.src_seq_len - len(src_input_ids) - 2, \\\n",
    "            self.tgt_seq_len - len(tgt_input_ids) - 1 # -2 for sos and eos token & -1 for sos token\n",
    "\n",
    "        if src_pad_len < 0 or tgt_pad_len < 0:\n",
    "            src_input_ids, tgt_input_ids = src_input_ids[:self.src_seq_len-2], \\\n",
    "                tgt_input_ids[:self.tgt_seq_len-1]\n",
    "            src_pad_len, tgt_pad_len = 0, 0\n",
    "        \n",
    "        src_input_ids, tgt_input_ids = torch.tensor(src_input_ids, dtype=torch.int64), \\\n",
    "            torch.tensor(tgt_input_ids, dtype=torch.int64)\n",
    "         \n",
    "        # concatenating sos, eos and pad tokens\n",
    "        src_input_ids = torch.cat(\n",
    "            [self.sos_token_src, src_input_ids, self.eos_token_src, self.pad_token_src.repeat(src_pad_len)])\n",
    "        label = torch.cat(\n",
    "            [tgt_input_ids, self.eos_token_tgt, self.pad_token_tgt.repeat(tgt_pad_len)])\n",
    "        tgt_input_ids = torch.cat(\n",
    "            [self.sos_token_tgt, tgt_input_ids, self.pad_token_tgt.repeat(tgt_pad_len)])\n",
    "        \n",
    "        # attention mask in encoder is 1 for all non-pad tokens and 0 for pad tokens\n",
    "        attention_mask_src = (src_input_ids != self.pad_token_src).int()\n",
    "        # casual attention mask in decoder is 1 for previous tokens and 0 for future tokens\n",
    "        casual_attention_mask_tgt = torch.triu(\n",
    "            torch.ones((self.tgt_seq_len, self.tgt_seq_len), dtype=torch.int64), diagonal=1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"encoder_input\": src_input_ids, # (seq_len,)\n",
    "            \"decoder_input\": tgt_input_ids, # (seq_len,)\n",
    "            \"encoder_mask\": attention_mask_src.unsqueeze(0).unsqueeze(0), # (seq_len,)\n",
    "            \"decoder_mask\": casual_attention_mask_tgt.unsqueeze(0), # (seq_len, seq_len)\n",
    "            \"label\" : label # (seq_len,)\n",
    "        }\n",
    "\n",
    "        \n",
    "class Seq2SeqDataLoader(pl.LightningDataModule):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer_src: str, tokenizer_tgt: str, \n",
    "                 src_lang: str, tgt_lang: str, src_seq_len: int, tgt_seq_len: int, \n",
    "                 batch_size: int, num_workers: int, split_size:int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.src_seq_len = src_seq_len\n",
    "        self.tgt_seq_len = tgt_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.split_size = split_size\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        self.train_df, self.val_df = train_test_split(\n",
    "            self.df, test_size=self.split_size, random_state=42\n",
    "        )\n",
    "        self.train_dataset = Seq2SeqDataset(\n",
    "            df=self.train_df, \n",
    "            tokenizer_src=self.tokenizer_src,\n",
    "            tokenizer_tgt=self.tokenizer_tgt, \n",
    "            src_lang=self.src_lang,\n",
    "            tgt_lang=self.tgt_lang,\n",
    "            src_seq_len=self.src_seq_len,\n",
    "            tgt_seq_len=self.tgt_seq_len)\n",
    "        \n",
    "        self.val_dataset = Seq2SeqDataset(\n",
    "            df=self.val_df, \n",
    "            tokenizer_src=self.tokenizer_src,\n",
    "            tokenizer_tgt=self.tokenizer_tgt, \n",
    "            src_lang=self.src_lang,\n",
    "            tgt_lang=self.tgt_lang,\n",
    "            src_seq_len=self.src_seq_len,\n",
    "            tgt_seq_len=self.tgt_seq_len)\n",
    "\n",
    "        print(len(self.train_dataset), len(self.val_dataset))\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "        \n",
    "\n",
    "with open(\"../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = config['train']\n",
    "dataset = Seq2SeqDataLoader(\n",
    "    df=pd.read_parquet(config['dataset_path']),\n",
    "    tokenizer_src=config['tokenizer']['src']['path'],\n",
    "    tokenizer_tgt=config['tokenizer']['tgt']['path'],\n",
    "    src_lang=config['tokenizer']['src']['lang'],\n",
    "    tgt_lang=config['tokenizer']['tgt']['lang'],\n",
    "    src_seq_len=config['tokenizer']['src']['seq_len'],\n",
    "    tgt_seq_len=config['tokenizer']['tgt']['seq_len'],\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    "    split_size=config['split_size']\n",
    ")\n",
    "dataset.setup()\n",
    "print(\"Dataset setup complete\")\n",
    "\n",
    "for batch in dataset.train_dataloader():\n",
    "    print(batch)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
